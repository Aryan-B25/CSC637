# -*- coding: utf-8 -*-
"""TwitterSentimentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JOa-Dsrq11pF-Ita09dB6JhNUe8VsPE_
"""

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

!pip install pandas numpy scikit-learn matplotlib

import pandas as pd
import numpy as np
import re
import string
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

train_df = pd.read_csv('twitter_training.csv')
test_df = pd.read_csv('twitter_validation.csv')

print("Training Data:\n", train_df.head())

train_df.columns = ["id", "user", "sentiment", "tweet"]
test_df.columns = ["id", "user", "sentiment", "tweet"]

train_df = train_df[['tweet', 'sentiment']]
test_df = test_df[['tweet', 'sentiment']]

sentiment_map = {"Positive": 1, "Negative": 0, "Neutral": 2}
train_df["sentiment"] = train_df["sentiment"].map(sentiment_map)
test_df["sentiment"] = test_df["sentiment"].map(sentiment_map)

print(train_df.head())

def clean_text(text):

  text = re.sub(r'http\S+', '', text)
  text = re.sub(r'@\w+', '', text)
  text = re.sub(r'#\w+', '', text)
  text = re.sub(r'\d+', '', text)
  text = text.translate(str.maketrans('', '', string.punctuation))
  text = text.lower()
  text = text.strip()
  return text

train_df["tweet"] = train_df["tweet"].fillna("")
test_df["tweet"] = test_df["tweet"].fillna("")

train_df["cleaned_tweet"] = train_df["tweet"].apply(clean_text)
test_df["cleaned_tweet"] = test_df["tweet"].apply(clean_text)

train_df = train_df[train_df["tweet"].str.strip() != '']
test_df = test_df[test_df["tweet"].str.strip() != '']

vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)

# Fit & Transform on Training data
X_train = vectorizer.fit_transform(train_df["tweet"])

# Transform Validation data
X_test = vectorizer.transform(test_df["tweet"])

y_train = train_df["sentiment"]
y_test = test_df["sentiment"]

model = MultinomialNB()
model.fit(X_train, y_train)

print("NaNs in Training Sentiment:", train_df['sentiment'].isna().sum())
print("NaNs in Testing Sentiment:", test_df['sentiment'].isna().sum())

train_df = train_df.dropna(subset=['sentiment'])
test_df = test_df.dropna(subset=['sentiment'])

vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)

X_train = vectorizer.fit_transform(train_df["tweet"])
X_test = vectorizer.transform(test_df["tweet"])

y_train = train_df["sentiment"]
y_test = test_df["sentiment"]

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=["Negative", "Positive", "Neutral"]))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=["Negative", "Positive", "Neutral"],
            yticklabels=["Negative", "Positive", "Neutral"], cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

import seaborn as sns

import matplotlib.pyplot as plt
import seaborn as sns  # Ensure this line is run again

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=["Negative", "Positive", "Neutral"],
            yticklabels=["Negative", "Positive", "Neutral"], cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

test_df["predicted_sentiment"] = y_pred
reverse_sentiment_map = {1: "Positive", 0: "Negative", 2: "Neutral"}
test_df["predicted_sentiment"] = test_df["predicted_sentiment"].map(reverse_sentiment_map)

test_df.to_csv("twitter_validation_predictions.csv", index=False)